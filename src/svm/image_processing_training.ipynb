{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rxhHWBO7wRW6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,confusion_matrix, precision_score, recall_score\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-Ln7uQMtb2Nd"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test  = 0,0,0,0\n",
        "df = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pVVCR90tbN_v"
      },
      "outputs": [],
      "source": [
        "def preprocess(df):\n",
        "    df = df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "def get_train_and_test(df):\n",
        "    # Prepare the data\n",
        "    X = df[['area', 'width', 'circularity', 'ellipticity', 'aspect ratio', 'form factor', 'perimeter',\n",
        "            'perimeter ratio of diameter', 'perimeter Ratio of Physiological Length and Physiological Width']]\n",
        "    y = df['Species']\n",
        "\n",
        "    # Perform Standard Scaling\n",
        "    standard_scaler = StandardScaler()\n",
        "    X_standard_scaled = standard_scaler.fit_transform(X)\n",
        "    # Split the data into training and testing sets\n",
        "    global X_train, X_test, y_train, y_test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_standard_scaled, y, test_size=0.1, random_state=42)\n",
        "\n",
        "    return standard_scaler.fit(X)\n",
        "\n",
        "\n",
        "def set_optimal_perimeters():\n",
        "    # Define the hyperparameter grid for tuning\n",
        "    # Define the range and step size\n",
        "    start = 1\n",
        "    stop = 100\n",
        "    step = 50\n",
        "\n",
        "    # Create the array using numpy's arange function\n",
        "    arr = np.arange(start, stop + step, step)\n",
        "    gamma = list(np.logspace(-3, 3, 20))\n",
        "    # param_grid = {\n",
        "    #   'C':arr,\n",
        "    #   'kernel': ['rbf'],\n",
        "    #   'degree': [2],\n",
        "    #   'gamma': ['scale', 'auto'] +gamma\n",
        "    #   #['scale', 'auto'] +gamma\n",
        "    # }\n",
        "\n",
        "    param_grid = {\n",
        "        'C': [101],\n",
        "        'kernel': ['rbf'],\n",
        "        'degree': [2],\n",
        "        'gamma': [2.976351441631316]\n",
        "        # ['scale', 'auto'] +gamma\n",
        "    }\n",
        "\n",
        "    return param_grid\n",
        "\n",
        "\n",
        "# Create a dataframe with the dependent and independent variables\n",
        "def print_metrics(y_test, y_pred):\n",
        "    # Evaluate the model on the test set\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print('Accuracy:', accuracy)\n",
        "    print('Precision:', precision)\n",
        "    print('Recall:', recall)\n",
        "\n",
        "\n",
        "def train(param_grid):\n",
        "    global X_train, y_train\n",
        "    # Define the SVM classifier\n",
        "    svm = SVC()\n",
        "\n",
        "    # Perform Grid Search Cross Validation\n",
        "    grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Print the best hyperparameters and corresponding accuracy\n",
        "    print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
        "    print(\"Best Accuracy: \", grid_search.best_score_)\n",
        "\n",
        "    # Train SVM with the best hyperparameters on the entire training set\n",
        "    best_svm = grid_search.best_estimator_\n",
        "\n",
        "    best_svm.fit(X_train, y_train)\n",
        "    # Evaluate the model on the test set\n",
        "    accuracy = best_svm.score(X_test, y_test)\n",
        "    # Predict labels for test data\n",
        "    y_pred = best_svm.predict(X_test)\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print_metrics(y_test, y_pred)\n",
        "\n",
        "    return best_svm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = '../../csv/features_data-Sheet1.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgf6AwSv2D_7",
        "outputId": "7cc630d1-60cf-41c6-c6f4-7213ab76443b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Hyperparameters:  {'C': 101, 'degree': 2, 'gamma': 2.976351441631316, 'kernel': 'rbf'}\n",
            "Best Accuracy:  0.9619273592656599\n",
            "Accuracy: 0.9808612440191388\n",
            "Precision: 0.9812104091765108\n",
            "Recall: 0.9808612440191388\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(file_path) \n",
        "preprocess(df)\n",
        "\n",
        "X_standard_scaled = get_train_and_test(df)\n",
        "param_grid = set_optimal_perimeters()\n",
        "\n",
        "best_svm = train(param_grid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['../../models/X_standard_scaler.pkl']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# Save the best SVM model\n",
        "joblib.dump(best_svm, '../../models/plant_prediction_model_svm.pkl')\n",
        "\n",
        "# Save standard scaler\n",
        "joblib.dump(X_standard_scaled, '../../models/X_standard_scaler.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(file_path) \n",
        "preprocess(df)\n",
        "X_standard_scaled = get_train_and_test(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9808612440191388\n",
            "Precision: 0.9812104091765108\n",
            "Recall: 0.9808612440191388\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "from sklearn import svm\n",
        "\n",
        "# Load the saved SVM model\n",
        "loaded_model = joblib.load('../../models/plant_prediction_model_svm.pkl')\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print_metrics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GySG0oT2WCtA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Class Labels:  [0.]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\anaconda3\\envs\\tf2.4\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "X_standard_scaler = joblib.load('../../models/X_standard_scaler.pkl')\n",
        "new_scaled_data = X_standard_scaler.transform(np.array([[70748.5,305.0,0.5519020363926462,1.908372278751408,1.2459016393442623,0.5519020363926462,1269.207272648811,3.067476552718655,1.8528573323340312]]))\n",
        "\n",
        "# Make predictions\n",
        "predictions = best_svm.predict(new_scaled_data)\n",
        "\n",
        "# Print the predicted class labels\n",
        "print(\"Predicted Class Labels: \", predictions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
